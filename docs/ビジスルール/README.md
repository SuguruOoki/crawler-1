# ビジネスルール
## 設計
「出力結果として何が欲しいか」が「目的」になるでしょう。スプレッドシートに特定の数値のみを反映するのか、他のシステムと連携するためのAPIを作るのか、自分のサイトで読み込むためのフィードを作るのか、出力結果のイメージを明確にしておくことで必要十分な設計が行えます。

 1. **収集**: ネットワーク上のデータにリクエストする。リンクがあれば収集し、さらにそれらのリンクに対してもリクエストする
 2. **解析**: パース。テキストデータを構造化データに変換する
 3. **抽出**: 対象データを構造化データの特定のキーから取り出したり、スクレイピングや正規表現を使って抽出する
 4. **加工**: 利用形態に合わせて、抽出したデータからノイズを取り除いたり、正規化したり、画像処理をしたりする
 5. **保存**: 収集したページや、抽出・加工したデータをデータストレージに保存する
 
## バッチ作成時の注意点

 - **工程ごとの分離**: 各工程は分離しているのが理想的です。
    - 対象ページが分割されていて、リクエストするURLが複数になる場合は「次のページ」を抽出するために、スクレイピングを伴う場合がありますが、目的ファイルを抽出するためのスクレイピング処理と分けておきましょう。
    - ネットワークリクエスト処理とスクレイピング処理が一緒になっていると、例えばスクレイピングに失敗してやり直す時に相手サーバーへのリクエストからやり直すことになり、相手サーバーに負荷がかかる上、開発効率も悪くなります。
    - また、あまり細かくリクエストとスクレイピングを分割しすぎると、パーツ間の連携の流れが見えにくくなり、保守が困難になるので、分離する粒度のバランスは必要ですが、最低限、すべての処理が同時に行われてしまう設計は避けましょう。
    - 各工程がうまく分割できていれば、Chapter5の05節で解説するクローラーを並行化する際にも対応しやすくなります。
 - **中間データを保存しておく**: 各工程を分割するために、ネットワークリクエストしたデータは保存するようにしたほうがよいでしょう
    - あるページのみ、スクレイピングに失敗した場合、もとのHTMLソースが手元にダウンロードされていれば、スクレイピングプログラムのみを修正して、再実行することで、ネットワークリクエストを省略してデバッグできます。
    - ネットワークリクエストは時間がかかるものなので、リクエストに失敗したページがあれば、失敗したページのみをリクエストし直せる仕組みにしておけば、データの補填を行う時も時間の短縮に繋がります。
    - また、どうしてもスクレイピングの対象データがプログラムで取れなかった場合、最悪、手動でそのデータのみダウンロードして、中間データを埋め合わせれば、その次の工程に回すことができます。
 - **実行時間と経過がわかるようにする**:
    - 対象サイトに何百ページもある場合、実行時間がわかると効率化を図れます。例えば、次回、同じようにクロールする時に、どのくらいの時間がかかりそうかがわかることで、クロールが何かの原因でいつもより時間がかかった時に、どこかのサイトでエラーが起きているかもしれないなどの判断に結びつけることができます。
    - クローラーの開発時は、クローリングの経過もわかるようにしたほうがよいでしょう。現在処理中のURLや工程のラベルを表示することで、自分の思った通りにプログラムが動いているか、または、思った通りに動いていないかが、わかります
    - クロール対象の状況をステータスとして保存しておくと後の判断に役立ちます。ネットワークリクエストにおいては、HTTPステータスコードを保存しておけば、例えば、対象データが取得できなかった場合、リンク切れだったのか、サーバーエラーが起きていたのか、メンテナンス中だったのかを後から判断することができます
 - **停止条件を明確にする**: 再帰的にリンクを取得したり、再帰的にスクレイピングしたりする場合は、停止条件を明確にしておかないと無限ループしてしまい、プログラムがいつまで待っても終わらなくなってしまいます
    - クロールする階層やドメインを限定したり、ダウンロードするファイル数の上限を決め、条件に応じてクロールを終了するようにしておくとよいでしょう。
 - **関数の引数をシンプルにする**: デバッグする際に、工程の処理を部分的に再現する必要が出てきます。この時、複雑なオブジェクトを関数の引数にしていると、オブジェクトを作る手間がかかり、そのオブジェクトを間違って作ってしまった場合、再現状況も変わってしまい、デバッグが難しくなります。
    - 中間データを保存しておき、そのキーを引数にするとよいでしょう。
 - **日時の扱いに注意する**: データベースへデータの更新日時を保存する場合、UTC（MEMO参照）にしておくとよいでしょう。クロール対象データに日時が含まれている場合、日本標準時（JST）で記載されているとは限りません。あるサイトはJST、あるサイトはUTCで日時データを返してくるのであれば、UTCに統一しておくことをおすすめします。
 - **新着の検知の仕組みを考える**:
    - クロールしたページへのリンクをすべて保存しておき、次回のクロール時に、保存されていないページのリンクが見つかれば、そのページを新着とみなす
    - リンク保存時に`{url: 'http://www.example.com/page/1', date: '2020-05-27 09:12:06'}`と保存しておき、「現在日時からt時間以内に保存されたデータ」を新着データとみなす


## 保守・運用
### 多重起動の防止

 1. バッチ起動時にプロセスファイルがある場合はexitする
 2. バッチ起動時にプロセスファイルを作成する
 3. バッチが終了したらプロセスファイルを削除する